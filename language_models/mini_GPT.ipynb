{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "data = open('../data/tiny_shakespeare.txt','r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "chrs_to_idx = {c: i for i,c in enumerate(chars)}\n",
    "idx_to_chrs = {i: c for i,c in enumerate(chars)}\n",
    "num_of_chrs = len(chars)\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [chrs_to_idx[c] for c in s]\n",
    "def decode(l):\n",
    "    print(l)\n",
    "    return ''.join(idx_to_chrs[i] for i in l)\n",
    "\n",
    "x = 'hello world'\n",
    "print(x)\n",
    "print(encode(x))\n",
    "print(decode(encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data = torch.tensor(encode(data))\n",
    "train_data = enc_data[:int(0.9*len(enc_data))]\n",
    "test_data = enc_data[int(0.9*len(enc_data)):]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "context_len = 8\n",
    "\n",
    "# print(enc_data[:100])\n",
    "# print(decode(enc_data[:100].tolist()))\n",
    "\n",
    "def get_batch(train=True):\n",
    "    data = train_data if train else test_data\n",
    "    idx = torch.randint(len(data) - context_len, (batch_size,))\n",
    "    x = torch.stack([data[i: i+context_len] for i in idx])\n",
    "    y = torch.stack([data[i+1: i+context_len+1] for i in idx])\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModule(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bigram_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor = None) -> tuple[torch.tensor, torch.tensor]:\n",
    "        logits = self.bigram_embedding(x)\n",
    "        loss = None\n",
    "        if y is not None: \n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T)) \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, context: torch.tensor, max_iter=100) -> torch.tensor:\n",
    "        with torch.no_grad():\n",
    "            curr_context = context\n",
    "            for _ in range(max_iter):\n",
    "                logits, _ = self.forward(context)\n",
    "                prob = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                new_token = torch.multinomial(prob, num_samples=1)\n",
    "                curr_context = torch.cat([curr_context, new_token], dim=1)\n",
    "\n",
    "            return curr_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 23, 61, 59, 49, 41, 60, 31, 55, 59, 59, 37, 27, 15, 63, 51, 46, 55, 33, 39, 49, 38, 10, 49, 39, 49, 29, 49, 22, 4, 46, 27, 49, 55, 27, 41, 44, 57, 55, 59, 0, 49, 7, 46, 15, 15, 49, 15, 26, 6, 44, 29, 33, 15, 7, 49, 15, 54, 31, 55, 15, 31, 61, 23, 31, 31, 27, 17, 6, 36, 49, 8, 29, 13, 59, 49, 19, 36, 36, 34, 31, 31, 49, 49, 3, 19, 31, 23, 33, 51, 55, 6, 61, 26, 15, 6, 22, 34, 31, 59, 23]\n",
      "\n",
      "KwukcvSquuYOCymhqUakZ:kakQkJ&hOkqOcfsqu\n",
      "k-hCCkCN,fQUC-kCpSqCSwKSSOE,Xk.QAukGXXVSSkk$GSKUmq,wNC,JVSuK\n",
      "tensor(4.6483, grad_fn=<NllLossBackward0>) tensor([[[ 0.6717, -1.6367, -0.8953,  ...,  0.2733,  0.5081,  1.6175],\n",
      "         [ 0.8026, -1.2532,  0.3793,  ..., -0.2087,  2.1832,  0.0637],\n",
      "         [ 0.7892, -0.5145,  0.0699,  ..., -0.4008,  0.7016, -0.4884],\n",
      "         ...,\n",
      "         [ 0.1261,  0.3174, -0.2498,  ..., -2.8524, -0.8535,  1.6397],\n",
      "         [ 0.5641,  0.4790, -0.1719,  ...,  1.4515, -0.8018,  1.2146],\n",
      "         [ 0.0250,  0.0942, -1.7039,  ...,  0.5054,  1.2448, -0.7690]],\n",
      "\n",
      "        [[-0.3797, -0.0627,  0.4836,  ..., -2.7740, -1.2045, -1.8788],\n",
      "         [-0.5207, -0.5870, -0.7502,  ..., -0.7758, -1.8988, -1.7450],\n",
      "         [ 0.6185,  0.0971,  0.7925,  ...,  1.8776,  0.0400, -0.0835],\n",
      "         ...,\n",
      "         [ 0.8026, -1.2532,  0.3793,  ..., -0.2087,  2.1832,  0.0637],\n",
      "         [-1.0469,  1.5115,  0.2380,  ...,  0.1674,  1.6021,  2.0010],\n",
      "         [-0.1930, -0.9409,  1.5673,  ..., -0.7353,  1.1126,  1.2690]],\n",
      "\n",
      "        [[-0.8817,  0.3507,  0.6449,  ..., -0.2164,  1.0210, -0.9813],\n",
      "         [-1.2749,  1.3809, -0.0907,  ..., -0.5831, -0.4079, -1.2671],\n",
      "         [ 0.0708,  0.1094,  1.3334,  ..., -0.1284,  1.2592,  0.4525],\n",
      "         ...,\n",
      "         [-0.5207, -0.5870, -0.7502,  ..., -0.7758, -1.8988, -1.7450],\n",
      "         [-0.1930, -0.9409,  1.5673,  ..., -0.7353,  1.1126,  1.2690],\n",
      "         [ 0.9477, -1.1367, -1.5746,  ...,  2.5285,  1.2272, -0.4007]],\n",
      "\n",
      "        [[-0.3444,  0.1497, -2.1176,  ...,  0.6134,  2.0009,  0.5238],\n",
      "         [ 0.1261,  0.3174, -0.2498,  ..., -2.8524, -0.8535,  1.6397],\n",
      "         [-0.1117, -1.5742, -1.5654,  ...,  1.8227,  1.6196,  0.8469],\n",
      "         ...,\n",
      "         [ 0.1261,  0.3174, -0.2498,  ..., -2.8524, -0.8535,  1.6397],\n",
      "         [-0.1930, -0.9409,  1.5673,  ..., -0.7353,  1.1126,  1.2690],\n",
      "         [-0.5207, -0.5870, -0.7502,  ..., -0.7758, -1.8988, -1.7450]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch()\n",
    "m = BigramModule(num_of_chrs)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long)).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'backwards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackwards\u001b[49m()\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      8\u001b[0m lossi\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'backwards'"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "for i in range(1000):\n",
    "    xb, yb = get_batch()\n",
    "    _, loss = m.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.item())\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
